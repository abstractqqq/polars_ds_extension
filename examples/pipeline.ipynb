{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from polars_ds.pipeline import Pipeline, Blueprint\n",
    "from polars.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builtin Pipeline Functions\n",
    "\n",
    "You can use it for:\n",
    "\n",
    "1. Data Science Pipelines\n",
    "2. Data preparation, manipulation, wrangling pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 17)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>Gender</th><th>City_Category</th><th>Employer_Category1</th><th>Employer_Category2</th><th>Monthly_Income</th><th>Customer_Existing_Primary_Bank_Code</th><th>Primary_Bank_Type</th><th>Contacted</th><th>Source_Category</th><th>Existing_EMI</th><th>Loan_Amount</th><th>Loan_Period</th><th>Interest_Rate</th><th>EMI</th><th>Var1</th><th>Approved</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;APPC90493171225&quot;</td><td>&quot;Female&quot;</td><td>&quot;A&quot;</td><td>&quot;A&quot;</td><td>4</td><td>2000.0</td><td>&quot;B001&quot;</td><td>&quot;P&quot;</td><td>&quot;N&quot;</td><td>&quot;G&quot;</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>0</td></tr><tr><td>&quot;APPD40611263344&quot;</td><td>&quot;Male&quot;</td><td>&quot;A&quot;</td><td>&quot;C&quot;</td><td>1</td><td>3500.0</td><td>&quot;B002&quot;</td><td>&quot;P&quot;</td><td>&quot;Y&quot;</td><td>&quot;G&quot;</td><td>0.0</td><td>20000</td><td>2</td><td>13.25</td><td>953</td><td>10</td><td>0</td></tr><tr><td>&quot;APPE70289249423&quot;</td><td>&quot;Male&quot;</td><td>&quot;C&quot;</td><td>&quot;C&quot;</td><td>4</td><td>2250.0</td><td>&quot;B003&quot;</td><td>&quot;G&quot;</td><td>&quot;Y&quot;</td><td>&quot;B&quot;</td><td>0.0</td><td>45000</td><td>4</td><td>null</td><td>null</td><td>0</td><td>0</td></tr><tr><td>&quot;APPF80273865537&quot;</td><td>&quot;Male&quot;</td><td>&quot;C&quot;</td><td>&quot;A&quot;</td><td>4</td><td>3500.0</td><td>&quot;B003&quot;</td><td>&quot;G&quot;</td><td>&quot;Y&quot;</td><td>&quot;B&quot;</td><td>0.0</td><td>92000</td><td>5</td><td>null</td><td>null</td><td>7</td><td>0</td></tr><tr><td>&quot;APPG60994436641&quot;</td><td>&quot;Male&quot;</td><td>&quot;A&quot;</td><td>&quot;A&quot;</td><td>4</td><td>10000.0</td><td>&quot;B001&quot;</td><td>&quot;P&quot;</td><td>&quot;Y&quot;</td><td>&quot;B&quot;</td><td>2500.0</td><td>50000</td><td>2</td><td>null</td><td>null</td><td>10</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 17)\n",
       "┌───────────────┬────────┬──────────────┬──────────────┬───┬──────────────┬──────┬──────┬──────────┐\n",
       "│ ID            ┆ Gender ┆ City_Categor ┆ Employer_Cat ┆ … ┆ Interest_Rat ┆ EMI  ┆ Var1 ┆ Approved │\n",
       "│ ---           ┆ ---    ┆ y            ┆ egory1       ┆   ┆ e            ┆ ---  ┆ ---  ┆ ---      │\n",
       "│ str           ┆ str    ┆ ---          ┆ ---          ┆   ┆ ---          ┆ i64  ┆ i64  ┆ i64      │\n",
       "│               ┆        ┆ str          ┆ str          ┆   ┆ f64          ┆      ┆      ┆          │\n",
       "╞═══════════════╪════════╪══════════════╪══════════════╪═══╪══════════════╪══════╪══════╪══════════╡\n",
       "│ APPC904931712 ┆ Female ┆ A            ┆ A            ┆ … ┆ null         ┆ null ┆ 0    ┆ 0        │\n",
       "│ 25            ┆        ┆              ┆              ┆   ┆              ┆      ┆      ┆          │\n",
       "│ APPD406112633 ┆ Male   ┆ A            ┆ C            ┆ … ┆ 13.25        ┆ 953  ┆ 10   ┆ 0        │\n",
       "│ 44            ┆        ┆              ┆              ┆   ┆              ┆      ┆      ┆          │\n",
       "│ APPE702892494 ┆ Male   ┆ C            ┆ C            ┆ … ┆ null         ┆ null ┆ 0    ┆ 0        │\n",
       "│ 23            ┆        ┆              ┆              ┆   ┆              ┆      ┆      ┆          │\n",
       "│ APPF802738655 ┆ Male   ┆ C            ┆ A            ┆ … ┆ null         ┆ null ┆ 7    ┆ 0        │\n",
       "│ 37            ┆        ┆              ┆              ┆   ┆              ┆      ┆      ┆          │\n",
       "│ APPG609944366 ┆ Male   ┆ A            ┆ A            ┆ … ┆ null         ┆ null ┆ 10   ┆ 0        │\n",
       "│ 41            ┆        ┆              ┆              ┆   ┆              ┆      ┆      ┆          │\n",
       "└───────────────┴────────┴──────────────┴──────────────┴───┴──────────────┴──────┴──────┴──────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet(\"../examples/dependency.parquet\").select(\n",
    "    pl.exclude([\"DOB\", \"Source\", \"Lead_Creation_Date\", \"City_Code\", \"Employer_Code\"])\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A data science pipeline with data science transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select\n",
    "*\n",
    ", 'TEST' as test_col\n",
    "from df\n",
    "where loan_period is not null\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blueprint name: example\n",
      "Column names: Lowercase all incoming columns.\n",
      "Blueprint current steps: 10\n",
      "Features Expected: ['id', 'gender', 'city_category', 'employer_category1', 'employer_category2', 'monthly_income', 'customer_existing_primary_bank_code', 'primary_bank_type', 'contacted', 'source_category', 'existing_emi', 'loan_amount', 'loan_period', 'interest_rate', 'emi', 'var1', 'approved']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a blueprint first.\n",
    "# A blueprint is a plan for a pipeline. No hard work will be done until the blueprint is materialized, which\n",
    "# is when the tranforms are fitted (e.g. scale learns the mean and std from base data)\n",
    "# If target is specified for the blueprint, target will be excluded from all transformations that require a fit,\n",
    "# and target will be auto-filled if the transformation requires a target field and when no target field is explicitly given.\n",
    "\n",
    "bp = (\n",
    "    Blueprint(\n",
    "        df, name=\"example\", target=\"approved\", lowercase=True\n",
    "    )  # You can optionally put target of the ML model here\n",
    "    .select_by_std(\n",
    "        min_ = 0.001, max_ = 30000\n",
    "    ) # keep only numeric features with std between min and max. This will drop `loan_amount` in this example\n",
    "    .sql_transform(sql)  # Run a SQL transform on the df\n",
    "    # Say you want to remove a population for your data pipeline.\n",
    "    .filter(\n",
    "        \"city_category is not null\"  # or equivalently, you can do: pl.col(\"city_category\").is_not_null()\n",
    "    )\n",
    "    # Here we explicitly put target, since this is not the target for prediction.\n",
    "    # Use a linear regression with x1 = var1, x2=existing_emi to predict missing values in loan_period\n",
    "    # This transform should not be used when memory is an issue.\n",
    "    .linear_impute(features=[\"var1\", \"existing_emi\"], target=\"loan_period\")\n",
    "    .impute([\"existing_emi\"], method=\"median\")\n",
    "    .with_columns(  # generate some features\n",
    "        pl.col(\"existing_emi\").log1p().alias(\"existing_emi_log1p\"),\n",
    "        pl.col(\"interest_rate\").log1p().alias(\"interest_rate_log1p\"),\n",
    "        pl.col(\"interest_rate\")\n",
    "        .clip(lower_bound=0, upper_bound=1000)\n",
    "        .alias(\"interest_rate_log1p_clipped\"),\n",
    "        pl.col(\"interest_rate\").sqrt().alias(\"interest_rate_sqrt\"),\n",
    "        pl.col(\"interest_rate\").shift(-1).alias(\"interest_rate_lag_1\"),  # any kind of lag transform\n",
    "    )\n",
    "    .scale(  # target is numerical, but will be excluded automatically because bp is initialzied with a target\n",
    "        cs.numeric().exclude([\"var1\", \"existing_emi_log1p\"]), method=\"standard\"\n",
    "    )  # Scale the columns up to this point. The columns below won't be scaled\n",
    "    .with_columns(\n",
    "        # Add missing flags\n",
    "        pl.col(\"employer_category1\").is_null().cast(pl.UInt8).alias(\"employer_category1_is_missing\")\n",
    "    )\n",
    "    .ordinal_encode(cols=[\"city_category\"])\n",
    "    .woe_encode(\n",
    "        cols=pl.exclude(\"id\")\n",
    "    )  # No need to specify target because we initialized bp with a target. None means encode all str columns\n",
    "    # .sort(by = \"monthly_income\", descending=True)\n",
    "    # .one_hot_encode(cols=None, drop_first=True) # None means all str columns, or you can provide a list of columns\n",
    "    # .target_encode(\"employer_category1\", min_samples_leaf = 20, smoothing = 10.0) # same as above\n",
    ")\n",
    "\n",
    "print(bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(name='example', feature_names_in_=['id', 'gender', 'city_category', 'employer_category1', 'employer_category2', 'monthly_income', 'customer_existing_primary_bank_code', 'primary_bank_type', 'contacted', 'source_category', 'existing_emi', 'loan_amount', 'loan_period', 'interest_rate', 'emi', 'var1', 'approved'], feature_names_out_=['id', 'gender', 'city_category', 'employer_category1', 'employer_category2', 'customer_existing_primary_bank_code', 'primary_bank_type', 'contacted', 'source_category', 'existing_emi', 'loan_period', 'interest_rate', 'emi', 'var1', 'approved', 'test_col', 'existing_emi_log1p', 'interest_rate_log1p', 'interest_rate_log1p_clipped', 'interest_rate_sqrt', 'interest_rate_lag_1', 'employer_category1_is_missing'], transforms=[<polars_ds.pipeline._step.ExprStep object at 0x7f29f44203a0>, <polars_ds.pipeline._step.SQLStep object at 0x7f29f448dea0>, <polars_ds.pipeline._step.ExprStep object at 0x7f29f448f5e0>, <polars_ds.pipeline._step.ExprStep object at 0x7f29f4461330>, <polars_ds.pipeline._step.ExprStep object at 0x7f29f45c40d0>, <polars_ds.pipeline._step.ExprStep object at 0x7f29f448f610>, <polars_ds.pipeline._step.ExprStep object at 0x7f29f4587f70>, <polars_ds.pipeline._step.ExprStep object at 0x7f29f45c5c90>, <polars_ds.pipeline._step.ExprStep object at 0x7f2a22c12b30>, <polars_ds.pipeline._step.ExprStep object at 0x7f2a11dac310>], ensure_features_in=False, ensure_features_out=True, lowercase=True, uppercase=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Materialize the blueprint\n",
    "pipe: Pipeline = bp.materialize(\n",
    "    # This is an optional parameter, which will be passed to .collect()\n",
    "    # when there is a fit step. User may decide which Polars optimization to use\n",
    "    optimizations=pl.QueryOptFlags()\n",
    ")\n",
    "# Text representation of the pipeline\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the blueprint, also return the entire plan for df as `_df_lazy`\n",
    "_df_lazy, _pipe = bp.materialize(\n",
    "    return_df = True\n",
    ")\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want separation between features (X) and target (y)\n",
    "# you can run the following:\n",
    "# df_x, df_y = pipe.transform(df, separate=True)\n",
    "\n",
    "df_transformed = pipe.transform(\n",
    "    df,\n",
    "    # This is an optional parameter, which can be used to tune performance\n",
    "    # during collect\n",
    "    optimizations=pl.QueryOptFlags(),\n",
    ")\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tuple(int(v) for v in pl.__version__.split(\".\")) >= (1, 34, 0):\n",
    "    batches = []\n",
    "    for df_batch in pipe.transform(df, return_lazy=True).collect_batches():\n",
    "        batches.append(df_batch)\n",
    "        # Pass the batch to any ML model which can be updated. (Online training)\n",
    "\n",
    "    # Here we test that the df combined from the batches is equivalent to the original df_transformed\n",
    "    # up to reordering. If we don't sort by id, the frames are equal in terms of all the transformed records\n",
    "    # but may not be equal in terms of the ordering.\n",
    "    # The reason is that when collecting from batches, ordering may not be\n",
    "    # so clear and strictly the same as the original. By testing, the `sort` transform should\n",
    "    # be avoided if you want the same df output with the same ordering without manually ordering them.\n",
    "\n",
    "    df_transformed_from_batches = pl.concat(batches).sort(\"id\")\n",
    "    assert_frame_equal(df_transformed.sort(\"id\"), df_transformed_from_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty. Because we filtered this to not null.\n",
    "df_transformed.filter(pl.col(\"city_category\").is_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A data preparation, manipulation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp2 = (\n",
    "    Blueprint(\n",
    "        df, name=\"example\", target=\"approved\", lowercase=True\n",
    "    )  # You can optionally put target of the ML model here\n",
    "    .filter(\n",
    "        \"city_category is not null\"  # or equivalently, you can do: pl.col(\"city_category\").is_not_null()\n",
    "    )\n",
    "    .with_columns(  # generate some features\n",
    "        pl.col(\"existing_emi\").log1p().alias(\"existing_emi_log1p\"),\n",
    "        pl.col(\"loan_amount\").log1p().alias(\"loan_amount_log1p\"),\n",
    "        pl.col(\"loan_amount\")\n",
    "        .clip(lower_bound=0, upper_bound=1000)\n",
    "        .alias(\"loan_amount_log1p_clipped\"),\n",
    "        pl.col(\"loan_amount\").sqrt().alias(\"loan_amount_sqrt\"),\n",
    "        pl.col(\"loan_amount\").shift(-1).alias(\"loan_amount_lag_1\"),  # any kind of lag transform\n",
    "    )\n",
    "    .group_by_agg(\n",
    "        by=\"city_category\",\n",
    "        agg=[\n",
    "            pl.col(\"loan_amount\").sqrt().mean().alias(\"loan_amount_sqrt_mean\"),\n",
    "            pl.col(\"loan_amount\").min().alias(\"loan_amount_min\"),\n",
    "            pl.col(\"loan_amount\").max().alias(\"loan_amount_max\"),\n",
    "        ],\n",
    "    )\n",
    "    .sort(by=[\"city_category\"], descending=True)\n",
    ")\n",
    "\n",
    "print(bp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = bp2.materialize()\n",
    "df_transformed2 = pipe2.transform(df)\n",
    "df_transformed2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app = pl.read_csv(\"apple_stock.csv\", try_parse_dates=True)\n",
    "df_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp3 = (\n",
    "    Blueprint(df_app)\n",
    "    .sort(by=[\"Date\"], descending=False)\n",
    "    .group_by_dynamic_agg(\n",
    "        index_column=\"Date\",\n",
    "        every=\"1y\",\n",
    "        agg=[\n",
    "            pl.col(\"Close\").sqrt().mean().alias(\"sqrt_mean\"),\n",
    "            pl.col(\"Close\").min().alias(\"min\"),\n",
    "            pl.col(\"Close\").max().alias(\"max\"),\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe3 = bp3.materialize()\n",
    "df_transformed3 = pipe3.transform(df_app)\n",
    "df_transformed3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Time Series Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "df_ts = pl.DataFrame(\n",
    "    {\n",
    "        \"id\": [1] * 9 + [2] * 15 + [3] * 6,\n",
    "        \"timestamp\": list(range(9)) + list(range(15)) + list(range(6)),\n",
    "        \"var_1\": np.random.rand(30),\n",
    "        \"var_2\": np.random.rand(30),\n",
    "        \"target\": random.choices([False, True], k=30),\n",
    "    }\n",
    ")\n",
    "\n",
    "bp4 = (\n",
    "    Blueprint(df_ts)\n",
    "    .scale([\"var_1\", \"var_2\"], method=\"standard\")\n",
    "    .group_by_agg(\n",
    "        by=\"id\",\n",
    "        maintain_order=True,\n",
    "        agg=[\n",
    "            \"timestamp\",\n",
    "            pl.col(\"var_1\").mul(10),\n",
    "            pl.col(\"var_2\").truediv(10),\n",
    "            \"target\",\n",
    "        ],\n",
    "    )\n",
    "    .explode(columns=pl.exclude(\"id\"))\n",
    "    .group_by_dynamic_agg(\n",
    "        index_column=\"timestamp\",\n",
    "        every=\"3i\",\n",
    "        group_by=\"id\",\n",
    "        start_by=\"datapoint\",\n",
    "        agg=[\n",
    "            pl.concat_list(\"var_1\", \"var_2\").alias(\"features\"),\n",
    "            pl.col(\"target\").sum(),\n",
    "        ],\n",
    "    )\n",
    "    .with_columns(pl.col(\"features\").cast(pl.Array(pl.Float64, (3, 2))))\n",
    ")\n",
    "\n",
    "pipe4 = bp4.materialize()\n",
    "df_transformed4 = pipe4.transform(df_ts)\n",
    "df_transformed4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization Methods\n",
    "\n",
    "You can always use a pickle to preserve the pipeline. So I won't demonstrate that here. What's more exciting is that PDS pipelines can be turned into JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.to_json(\"test.json\")\n",
    "pipe_reload = Pipeline.from_json(open(\"test.json\").read())\n",
    "# True\n",
    "assert_frame_equal(df_transformed, pipe_reload.transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.to_json(\"test2.json\")\n",
    "pipe2_reload = Pipeline.from_json(open(\"test2.json\").read())\n",
    "# True\n",
    "assert_frame_equal(df_transformed2, pipe2_reload.transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3.to_json(\"test3.json\")\n",
    "pipe3_reload = Pipeline.from_json(open(\"test3.json\").read())\n",
    "# True\n",
    "assert_frame_equal(df_transformed3, pipe3_reload.transform(df_app))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe4.to_json(\"test4.json\")\n",
    "pipe4_reload = Pipeline.from_json(open(\"test4.json\").read())\n",
    "# True\n",
    "assert_frame_equal(df_transformed4, pipe4_reload.transform(df_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tranformations in Pipeline\n",
    "\n",
    "Need version >= v0.4.6 (Not released yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"../examples/dependency.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "# Any custom function must satistfy the following function signature:\n",
    "# func(df:Union[pl.DataFrame, pl.LazyFrame], cols: List[str], ...) -> List[pl.Expr]\n",
    "# where ... means kwargs\n",
    "# Here is a custom imputer\n",
    "\n",
    "\n",
    "def smallest_abs_impute(\n",
    "    df: Union[pl.DataFrame, pl.LazyFrame], cols: List[str], epsilon: float = 0.01\n",
    ") -> List[pl.Expr]:\n",
    "    \"\"\"\n",
    "    Imputes columns by the min of the absolute values for c in columns, plus epsilon.\n",
    "    \"\"\"\n",
    "    temp = df.lazy().select(pl.col(cols).abs().min() + epsilon).collect().row(0)\n",
    "    return [pl.col(c).fill_null(m) for c, m in zip(cols, temp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = (\n",
    "    Blueprint(df, name=\"example\", target=\"approved\", lowercase=True).append_fit_func(\n",
    "        smallest_abs_impute, [\"var1\", \"existing_emi\", \"loan_amount\"], epsilon=0.5\n",
    "    )\n",
    "    # Use append_fit_func for custom transforms\n",
    ")\n",
    "# Notice that the value to impute is correct, it is 0.5, because the min abs of the columns are 0.\n",
    "pipe: Pipeline = bp.materialize()\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.transform(df).null_count().select([\"var1\", \"existing_emi\", \"loan_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scriptable Steps in Pipeline\n",
    "\n",
    "What is a scriptable step? It means we can encode steps in a json or yaml file easily. As long as we can turn the text into \n",
    "a valid Python dictionary, the step (the transformation) can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(pl.col(\"Existing_EMI\").null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = Blueprint(df, name=\"example\", target=\"approved\")\n",
    "\n",
    "# Takes in a dict with 3 fields: `name`, `args`, and `kwargs`.\n",
    "# Args and kwargs are optional depending on whether the method call needs certain arguments.\n",
    "step_dict_1 = {\"name\": \"impute\", \"kwargs\": {\"cols\": [\"Existing_EMI\"], \"method\": \"median\"}}\n",
    "\n",
    "step_dict_2 = {\"name\": \"does_not_exist\", \"kwargs\": {\"test\": 1}}\n",
    "\n",
    "# filter_step = {\n",
    "#     \"name\": \"filter\",\n",
    "#     \"args\": [\"Employer_Category1 is not null\"]\n",
    "# }\n",
    "\n",
    "bp.append_step_from_dict(step_dict_1)\n",
    "\n",
    "# .append_step_from_dict(\n",
    "#     filter_step\n",
    "# )\n",
    "\n",
    "# bp.append_step_from_dict(step_dict_2) # Will error\n",
    "pipe = bp.materialize()\n",
    "#\n",
    "df_transformed = pipe.transform(df)\n",
    "df_transformed.select(\n",
    "    pl.col(\"Existing_EMI\").null_count()  # Imputed. So 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transforms as a Scriptable Steps in Pipeline\n",
    "\n",
    "You need to inherit the blueprint class. Once the blueprint is materialized (learned). You do not need this class any more, because the \"learned\" info should all be encoded as Polars expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars_ds.pipeline import Blueprint, FitStep\n",
    "from typing import Union, List\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def smallest_abs_impute(\n",
    "    df: Union[pl.DataFrame, pl.LazyFrame], cols: List[str], epsilon: float = 0.01\n",
    ") -> List[pl.Expr]:\n",
    "    \"\"\"\n",
    "    Imputes columns by the min of the absolute values for c in columns, plus epsilon.\n",
    "    \"\"\"\n",
    "    temp = df.lazy().select(pl.col(cols).abs().min() + epsilon).collect().row(0)\n",
    "    return [pl.col(c).fill_null(m).name.suffix(\"_imputed\") for c, m in zip(cols, temp)]\n",
    "\n",
    "\n",
    "class ExtendedBlueprint(Blueprint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def smallest_abs_impute(self, cols: List[str], epsilon: float = 0.01) -> \"ExtendedBlueprint\":\n",
    "        # bind all arguments, except df and cols.\n",
    "        # If you don't want to use partial from functool, you can define an inner function\n",
    "        partial_func = partial(smallest_abs_impute, epsilon=epsilon)\n",
    "        self._steps.append(FitStep(partial_func, cols, self.exclude))\n",
    "        return self\n",
    "\n",
    "    def smallest_abs_impute2(self, cols: List[str], epsilon: float = 0.01) -> \"ExtendedBlueprint\":\n",
    "        # bind all arguments, except df and cols.\n",
    "        # Example of using an inner function\n",
    "        def inner_func(df: Union[pl.DataFrame, pl.LazyFrame], cols: List[str]) -> List[pl.Expr]:\n",
    "            temp = df.lazy().select(pl.col(cols).abs().min() + epsilon).collect().row(0)\n",
    "            return [pl.col(c).fill_null(m).name.suffix(\"_imputed2\") for c, m in zip(cols, temp)]\n",
    "\n",
    "        self._steps.append(FitStep(inner_func, cols, self.exclude))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = ExtendedBlueprint(df, name=\"example\", target=\"approved\")\n",
    "\n",
    "# Takes in a dict with 3 fields: `name`, `args`, and `kwargs`.\n",
    "# Args and kwargs are optional depending on whether the method call needs certain arguments.\n",
    "step_dict_1 = {\"name\": \"smallest_abs_impute\", \"kwargs\": {\"cols\": [\"Existing_EMI\"], \"epsilon\": 0.01}}\n",
    "\n",
    "step_dict_2 = {\n",
    "    \"name\": \"smallest_abs_impute2\",\n",
    "    \"kwargs\": {\"cols\": [\"Existing_EMI\"], \"epsilon\": 0.01},\n",
    "}\n",
    "\n",
    "bp.append_step_from_dict(step_dict_1).append_step_from_dict(step_dict_2)\n",
    "\n",
    "pipe = bp.materialize()\n",
    "df_transformed = pipe.transform(df)\n",
    "\n",
    "df_transformed.with_columns(impute_value=pl.col(\"Existing_EMI\").abs().min() + 0.01).filter(\n",
    "    pl.col(\"Existing_EMI\").is_null()\n",
    ").select(\n",
    "    pl.col(\"Existing_EMI\"),\n",
    "    pl.col(\"Existing_EMI_imputed\"),\n",
    "    pl.col(\"Existing_EMI_imputed2\"),\n",
    "    pl.col(\"impute_value\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping PDS Pipeline (blueprint) Inside a Sklearn Pipeline\n",
    "\n",
    "It is not recommended, but it is possible. If there is a sklearn pipeline transform you want, and that is not implemented in PDS, please submit a feature request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars_ds.pipeline as pm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class CustomPDSTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.pipe = None\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        # specify all the rules for the transform here\n",
    "        bp = (\n",
    "            pm.Blueprint(df, name=\"example\", target=\"approved\", lowercase=True)\n",
    "            .filter(\n",
    "                \"city_category is not null\"  # or equivalently, you can do: pl.col(\"city_category\").is_not_null()\n",
    "            )\n",
    "            .select(cs.numeric() | cs.by_name([\"gender\", \"employer_category1\", \"city_category\"]))\n",
    "            .linear_impute(features=[\"var1\", \"existing_emi\"], target=\"loan_period\")\n",
    "            .impute([\"existing_emi\"], method=\"median\")\n",
    "        )\n",
    "        self.pipe = bp.materialize()\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        return self.pipe.transform(df)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "df = pl.read_parquet(\"../examples/dependency.parquet\")\n",
    "\n",
    "pipe = Pipeline(steps=[(\"CustomPDSTransformer\", CustomPDSTransformer())])\n",
    "df_transformed = pipe.fit_transform(df)\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
